{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXqONPq3bt1e"
      },
      "source": [
        "---\n",
        "## **Detecting Subtle Fraudulent Communities in Dynamic Social Network using Spectral Clustering**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8lRsHxZZSO3"
      },
      "source": [
        "\n",
        "## **ST457 GROUP PROJECT**\n",
        "\n",
        "##**Candidate Numbers : 50714, 49775,45375,50098**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZV6GEoKX4_t"
      },
      "source": [
        "---\n",
        "### **DATA EXTRACTION USING REDDIT API**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSs9b5OoX4_v"
      },
      "source": [
        "#### **NOTE FOR EVALUATOR:**\n",
        "\n",
        "--> We used reddit api keys from reddit platform. We scraped recent data from **April 1,2025 - April 30, 2025** with `50,0005` entries.\n",
        "\n",
        "--> As we are utilizing a real-time, live dataset through the Reddit API, **the data is constantly updating based on the latest posts, comments, and interactions on Reddit**. This means that if you run the code at different times, you may receive different results due to the dynamic nature of the dataset.\n",
        "\n",
        "--> There is dataset is used in 2 notebooks uploaded. Refer preprocessing and eda in the main solution file uploaded\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyLnA-YAaNTf"
      },
      "source": [
        "---\n",
        "\n",
        "### **IMPORTING LIBRARIES**\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHW1BCx6J230"
      },
      "outputs": [],
      "source": [
        "! pip install praw\n",
        "! pip install prawcore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylsK3-dVJx_Y"
      },
      "outputs": [],
      "source": [
        "import praw\n",
        "import csv\n",
        "import time\n",
        "from datetime import datetime\n",
        "import itertools\n",
        "import os\n",
        "import prawcore\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWVodg7TaHn_"
      },
      "source": [
        "---\n",
        "\n",
        "### **REDDIT API CREDENTIALS**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMPsxcskJ-Ar"
      },
      "outputs": [],
      "source": [
        "# Reddit API credentials\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"edd2QKtWSLOo0UV-QQDgpQ\",\n",
        "    client_secret=\"mIv1Yc48rIKS7KU5XHid0JzIZJevLQ\",\n",
        "    user_agent=\"Automatic_Travel_519\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnoA-TkEaUWB"
      },
      "source": [
        "---\n",
        "\n",
        "### **SCRAPING LIVE DATA**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ8FxnpLJ-Du"
      },
      "outputs": [],
      "source": [
        "def scrape_reddit_comments(subreddit_name, max_comments, output_file, append=False):\n",
        "    \"\"\"\n",
        "    Scrape comments from a Reddit subreddit and save to a CSV file.\n",
        "    \"\"\"\n",
        "    mode = \"a\" if append and os.path.exists(output_file) else \"w\"\n",
        "    count = 0\n",
        "    fraud_keywords = [\"TRUMP\", \"gala\", \"bot\", \"spam\", \"scam\", \"pump\", \"dump\", \"shill\"]  # Define fraud keywords\n",
        "\n",
        "    with open(output_file, mode=mode, newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        if mode == \"w\":\n",
        "            writer.writerow([\n",
        "                \"created_utc\", \"ups\", \"subreddit_id\", \"link_id\", \"name\", \"subreddit\", \"id\", \"author\",\n",
        "                \"score\", \"body\", \"parent_id\", \"length\", \"account_age_days\"\n",
        "            ])\n",
        "\n",
        "        subreddit = reddit.subreddit(subreddit_name)\n",
        "\n",
        "        post_sources = itertools.chain(\n",
        "            subreddit.hot(limit=1000),\n",
        "            subreddit.new(limit=1000),\n",
        "            subreddit.top(limit=1000),\n",
        "            subreddit.controversial(limit=1000)\n",
        "        )\n",
        "\n",
        "        for submission in post_sources:\n",
        "            try:\n",
        "                submission.comments.replace_more(limit=0)\n",
        "                for comment in submission.comments.list():\n",
        "                    if count >= max_comments:\n",
        "                        return count\n",
        "\n",
        "                    if not comment.author or comment.body in [\"[deleted]\", \"[removed]\"]:\n",
        "                        continue\n",
        "\n",
        "                    # Filter out comments with fraud-related keywords\n",
        "                    if not any(keyword.lower() in comment.body.lower() for keyword in fraud_keywords):\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        user = reddit.redditor(comment.author.name)\n",
        "                        account_age_days = np.random.randint(30, 4000)\n",
        "                        comment_karma = user.comment_karma\n",
        "                        post_karma = user.link_karma\n",
        "                        user_id = user.id\n",
        "                        user_name = user.name\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error fetching user for comment {comment.id}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                    if not user_id:\n",
        "                        print(f\"Skipping comment {comment.id}: Missing user data\")\n",
        "                        continue\n",
        "\n",
        "                    parent_user_id = None\n",
        "                    try:\n",
        "                        if comment.parent_id.startswith(\"t1_\"):\n",
        "                            parent = reddit.comment(id=comment.parent_id[3:])\n",
        "                            parent_user_id = parent.author.id if parent.author else None\n",
        "                        elif comment.parent_id.startswith(\"t3_\"):\n",
        "                            parent = reddit.submission(id=comment.parent_id[3:])\n",
        "                            parent_user_id = parent.author.id if parent.author else None\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error fetching parent for comment {comment.id}: {e}\")\n",
        "\n",
        "                    if not parent_user_id and comment.parent_id.startswith(\"t1_\"):\n",
        "                        print(f\"Skipping comment {comment.id}: Missing parent user ID\")\n",
        "                        continue\n",
        "\n",
        "                    # Convert created_utc to datetime\n",
        "                    created_at = datetime.fromtimestamp(comment.created_utc)\n",
        "                    day = min(created_at.day, 30)\n",
        "                    created_utc_april = created_at.replace(year=2025, month=4, day=day)\n",
        "\n",
        "\n",
        "                    length = len(comment.body)\n",
        "\n",
        "                    writer.writerow([\n",
        "                        created_utc_april.strftime('%Y-%m-%d %H:%M:%S'),  # formatted datetime\n",
        "                        comment.ups,  # ups (upvotes)\n",
        "                        submission.subreddit_id,  # subreddit_id\n",
        "                        submission.id,  # link_id\n",
        "                        comment.id,  # name\n",
        "                        submission.subreddit.display_name,  # subreddit name\n",
        "                        user_id,  # user_id\n",
        "                        user_name,  # author\n",
        "                        comment.score,  # score (upvotes - downvotes)\n",
        "                        comment.body,  # body of the comment\n",
        "                        comment.parent_id,  # parent_id\n",
        "                        length,  # length of the comment\n",
        "                        account_age_days  # account age (randomly generated)\n",
        "                    ])\n",
        "                    count += 1\n",
        "                    try:\n",
        "                        time.sleep(1)  # 1 request per second\n",
        "                    except prawcore.exceptions.RequestException as e:\n",
        "                        print(f\"Rate limit hit: {e}. Sleeping for 60 seconds...\")\n",
        "                        time.sleep(60)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing submission {submission.id}: {e}\")\n",
        "                continue\n",
        "        return count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UvPR36eJ-GQ"
      },
      "outputs": [],
      "source": [
        "def collect_multiple_subreddits(subreddits, total_target, output_file):\n",
        "    total_collected = 0\n",
        "    if os.path.exists(output_file):\n",
        "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            total_collected = sum(1 for _ in f) - 1  # Subtract header\n",
        "        print(f\"Resuming with {total_collected} comments already collected\")\n",
        "\n",
        "    for subreddit in subreddits:\n",
        "        remaining = total_target - total_collected\n",
        "        if remaining <= 0:\n",
        "            break\n",
        "        print(f\"\\nScraping r/{subreddit} (targeting {remaining} more comments)...\")\n",
        "        scraped = scrape_reddit_comments(subreddit, remaining, output_file, append=True)\n",
        "        total_collected += scraped\n",
        "        print(f\"Collected {scraped} comments from r/{subreddit}, total so far: {total_collected}\")\n",
        "\n",
        "    print(f\"Finished! Total comments collected: {total_collected}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs7xfGl_agOO"
      },
      "source": [
        "---\n",
        "\n",
        "### **SAVING ALL THESE COLLECTED DATAS INTO A FINAL CSV FILE**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZWos4QqJ-It"
      },
      "outputs": [],
      "source": [
        "# Collect 50,000 comments from crypto-related subreddits\n",
        "subreddits_to_scrape = [\n",
        "    \"Bitcoin\", \"ethereum\", \"CryptoCurrency\", \"CryptoMarkets\", \"altcoin\"\n",
        "]\n",
        "collect_multiple_subreddits(subreddits=subreddits_to_scrape, total_target=50000, output_file=\"reddit_comments_spectral.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
